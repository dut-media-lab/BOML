Quickly build your bilevel meta-learning model
===============================================
  - Core Modules: <div3 id="a31"></div3>
    1. data_loader
       - Related: 
            - boml.data_loader.meta_omniglot <br>
            - boml.data_loader.meta_mini_imagenet <br>
            - boml.data_loader.mnist <br>
        ```
        boml.meta_omniglot(
            folder=DATA_FOLDER, 
            std_num_classes=None, 
            examples_train=None, 
            examples_test=None, 
            one_hot_enc=True, 
            _rand=0, 
            n_splits=None)
        boml.meta_mini_imagenet(
          folder=DATA_FOLDER, 
          sub_folders=None, 
          std_num_classes=None,
          examples_train=None, 
          examples_test=None, 
          resize=84, 
          one_hot_enc=True, 
          load_all_images=True,
          h5=False):
        ```
        boml.data_loader manages different datasets and generate bathes of tasks for training and testing.
       - Args：<br>
            - folder: root folder name. Use os module to modify the path to the datasets<br>
            - std_num_classes: standard number of classes for N-way classification<br>
             - examples_train:standard number of examples to be picked in each generated per classes for training (eg .1 shot, examples_train=1)<br>
            - examples_test: standard number of examples to be picked in each generated per classes for testing
            - one_hot_enc: one hot encoding<br>
            - _rand: random seed or RandomState for generate training, validation, testing meta-datasets split<br>
            - n_splits: num classes per split<br>
       - Usage:
          ```
          dataset = boml.meta_omniglot(args.num_classes,
                                  (args.num_examples, args.examples_test))
          ```
       - Returns: an initialized instance of data loader 
    2. Experiment
       - Aliases: 
           - boml.data_loader.Experiment
        ```
        boml.Experiment(
            dataset=None, 
            dtype=tf.float32)
        ```
        boml.Experiment manages inputs, outputs and task-specific parameters.
       - Args:
          - dataset: initialized instance of data_loader<br>
          - dtype: default tf.float32<br>
       - Attributes:<br>
          - x: input placeholder of input for your defined lower level problem<br>
          - y: label placeholder of output for yourdefined lower level problem<br>
          - x_:input placeholder of input for your defined upper level problem<br>
          - y_:label placeholder of output for your defined upper level problem<br>
          - model: used to restore the task-specific model <br>
          - errors: dictionary to restore defined loss functions of different levels<br> 
          - scores: dictionary to restore defined accuracies functions<br> 
          - optimizers: dictonary to restore optimized chosen for inner and outer loop optimization<br>
       - Usage:
        ```
        ex = boml.Experiment(datasets = dataset)
        ex.errors['training'] = boml.utils.cross_entropy_loss(pred=ex.model.out, label=ex.y, method='HyperOptim')
        ex.scores['accuracy'] = tf.contrib.metrics.accuracy(tf.argmax(tf.nn.softmax(ex.model.out), 1), tf.argmax(ex.y, 1))
        ex.optimizers['apply_updates'], _ = boml.BOMLOptSGD(learning_rate=lr0).minimize(ex.errors['training'],var_list=ex.model.var_list)
        ```
       - Returns: an initialized instance of Experiment 
  
    3. BOMLOptimizer 
       - Aliases: 
         - boml.core.BOMLOptimizer
        ```
        boml.BOMLOptimizer(
            Method=None, 
            inner_method=None, 
            outer_method=None, 
            truncate_iter=-1,
            experiments=[]
            )
        ```
        BOMLOptimizer is the main class in `boml`, which takes responsibility for the whole process of model construnction and back propagation. 
       - Args:
          - Method: define basic method for following training process, it should be included in [`HyperOptim`, `BilevelOptim`], `HyperOptim` type includes methods like `MAML`, `FOMAML`, `TNet`, `WarpGrad`; `BilevelOptim` type includes methods like `BDA`, `RHG`, `TRHG`, `Implicit HG`, `DARTS`;<br>
          - inner_method: method chosen for solving LLproblem, including [`Trad` ,`Simple`, `Aggr`], `BilevelOptim` type choose either `Trad` for traditional optimization strategies or `Aggr` for Gradient Aggragation optimization 'HyperOptim' type should choose `Simple`, and set specific parameters for detailed method choices like FOMAML or TNet.<br>
          - outer_method: method chosen for solving LLproblem, including [`Reverse` ,`Simple`, `Forward`, `Implcit`], `HyperOptim` type should choose Simple, and set specific parameters for detailed method choices like `FOMAML`
          - truncate_iter: specific parameter for `Truncated RHG` method, defining number of iterations to truncate in the Back propagation process<br>
          - experiments: list of experiment objects that has already been initialized <br>
        - Usage:
        ```
        ex = boml.Experiment(boml.meta_omniglot(5,1,15))
        boml_ho = boml.BOMLOptimizer(
            Method='HyperOptim', 
            inner_method='Simple', 
            outer_method='Simple',
            experiments=ex)
        ```
       - Utility functions:
          - learning_rate(): returns defined inner learning rate
          - meta_learning_rate(): returns defined outer learning rate 
          - Method: return defined method type 
          - param_dict: return the dictionary that restores general parameters, like use_T,use_Warp, output shape of defined model, learn_lr, s, t, alpha, first_order.
       - Returns: an initialized instance of BOMLOptimizer
  - Core Built-in functions of BOMLOptimizer: <div3 id="a32"></div3> 
    1. BOMLOptimizer.Meta_model:
       - Aliases: 
         - boml.core.BOMLOptimizer.Meta_model()
        ```
        boml.core.BOMLOptimizer.Meta_model(
            _input, 
            dataset, 
            meta_model='v1', 
            name='Hyper_Net', 
            use_T=False, 
            use_Warp=False,
            **model_args
        )
        ```
       This method must be called once at first to build meta modules and initialize meta parameters and neural networks.
       - Args:
          - _input: orginal input for neural network construction;
          - dataset: which dataset to use for training and testing. It should be initialized before being passed into the function
          - meta_model: model chosen for neural network construction, `v1` for C4L with fully connected layer,`v2` for Residual blocks with fully connected layer.
          - name: name for Meta model modules used for BOMLNet initialization
          - use_T: whether to use T layer for C4L neural networks
    2. BOMLOptimizer.Base_model:
       - Aliases: 
          - boml.core.BOMLOptimizer.Base_model()
        ```
        boml.core.BOMLOptimizer.Base_model(
            _input, 
            meta_learner, name='Task_Net',
            weights_initializer=tf.zeros_initializer
        )
        ```
       This method has to be called for every experiment and takes responsibility for defining task-specific modules and inner optimizers.
       - Args:
          - _input: orginal input for neural network construction of task-specific module;
          - meta_learner: returned value of Meta_model function, which is a instance of BOMLNet or its child classes
          - name: name for Base model modules used for BOMLNet initialization
          - weights_initializer: initializer function for task_specific network, called by 'BilevelOptim' method
       - Returns: task-specific model part
    3. BOMLOptimizer.ll_problem:
       - Aliases: 
             - boml.core.BOMLOptimizer.ll_problem()
        ```
        boml.core.BOMLOptimizer.ll_problem(
              inner_objective,
              learning_rate, 
              T, 
              inner_objective_optimizer='SGD', 
              outer_objective=None,
              learn_lr=False, 
              alpha_init=0.0, 
              s=1.0, t=1.0, 
              learn_alpha=False, 
              learn_st=False,
              learn_alpha_itr=False, 
              var_list=None,
              init_dynamics_dict=None, 
              first_order=False, 
              loss_func=utils.cross_entropy_loss, 
              momentum=0.5,
              beta1=0.0,
              beta2=0.999,
              regularization=None, 
              experiment=None, 
              scalor=0.0, 
              **inner_kargs
        )
        ```
       After construction of neural networks, solutions to lower level problems should be regulated in ll_problem.
       - Args:
          - inner_objective: loss function for the inner optimization problem
          - learning_rate: step size for inner loop optimization
          - T: numbers of steps for inner gradient descent optimization
          - inner_objective_optimizer: Optimizer type for the outer parameters, should be in list [`SGD`,`Momentum`,`Adam`]
          - outer_objective: loss function for the outer optimization problem, which need to be claimed in BDA agorithm
          - alpha_init: initial value of ratio of inner objective to outer objective in BDA algorithm
          - s,t: coefficients of aggregation of inner and outer objectives in BDA algorithm, default to be 1.0
          - learn_alpha: specify parameter for BDA algorithm to decide whether to initialize alpha as a hyper parameter
          - learn_alpha_itr: parameter for BDA algorithm to specify whether to initialize alpha as a vector, of which every dimension's value is step-wise scale factor fot the optimization process        
          - learn_st: specify parameter for BDA algorithm to decide whether to initialize s and t as hyper parameters
          - first_order: specific parameter to define whether to use implement first order MAML, default to be `FALSE`
          - loss_func: specifying which type of loss function is used for the maml-based method, which should be consistent with the form to compute the inner objective
          - momentum: specific parameter for Optimizer.BOMLOptMomentum to set initial value of momentum
          - beta1, beta2: specific parameter for Optimizer.BOMLOptMomentum to set initial value of Adam
          - regularization: whether to add regularization terms in the inner objective 
          - experiment: instance of Experiment to use in the Lower Level Problem, especifially needed in the `HyperOptim` type of method.
          - scalor: coefficient of regularization term in the objective function.
          - var_list: optional list of variables (of the inner optimization problem)from
          - init_dynamics_dict: optional dictrionary that defines Phi_0 (see `OptimizerDict.set_init_dynamics`)
          - inner_kargs: optional arguments to pass to `boml.core.optimizer.minimize`
       - Returns: task-specific model part
   
    4. BOMLOptimizer.ul_problem
       - Aliases:
          - boml.core.BOMLOptimizer.ul_problem()
            ```
            boml.core.BOMLOptimizer.ul_Problem(
                outer_objective, 
                meta_learning_rate, 
                inner_grad,
                meta_param=None, 
                outer_objective_optimizer='Adam', 
                Reptile=False, 
                Darts=False, 
                epsilon=1.0,
                beta1=0.9,beta2=0.999, 
                momentum=0.5, 
                global_step=None
            )
            ```
        This method define upper level problems and choose optimizers to optimize meta parameters, which should be called afer ll_problem.
        - Args:
            - outer_objective: scalar tensor for the outer objective
            - meta_learning_rate: step size for outer loop optimization
            - inner_grad: Returned value of boml.BOMLOptimizer.LLProblem()
            - meta_param: optional list of outer parameters and model parameters
            - outer_objective_optimizer: Optimizer type for the outer parameters, should be in list [`SGD`,`Momentum`,`Adam`]
            - Reptile: BOOLEAN, specific parameters to define whether to implement `Reptile` algorithm
            - Darts: BOOLEAN, specific parameters to define whether to implement 'DARTS' algorithm
            - epsilon: Float, cofffecients to be used in DARTS algorithm
            - momentum: specific parameters to be used to initialize `Momentum` algorithm
            - beta1, beta2: specific parameters to be used to initialize `Adam`
            - global_step: optional global step. By default tries to use the last variable in the collection GLOBAL_STEP
        - Returns：meta_param list, used for debugging
    5. Aggregate_all:
       - Aliases: 
           - boml.core.BOMLOptimizer.Aggregate_all()
          ```
          boml.core.BOMLOptimizer.Aggregate_all(
              aggregation_fn=None, 
              gradient_clip=None
              )

          ```
       - Args:
          - aggregation_fn:Optional operation to aggregate multiple outer_gradients (for the same meta parameter),by (default: reduce_mean)
          - gradient_clip: optional operation to clip the aggregated outer gradients
       - Returns: None
     Finally, Aggregate_all has to be called to aggregate gradient of different tasks, and define operations to apply outer gradients and update meta parametes.
    6. run:
       - Aliases: 
          - boml.core.BOMLOptimizer.run()
        ```
        boml.core.BOMLOptimizer.run(
            inner_objective_feed_dicts=None,
            outer_objective_feed_dicts=None,
            train_batches=None,
            initializer_feed_dict=None, 
            session=None, 
            online=False,
            _skip_hyper_ts=False, 
            _only_hyper_ts=False, 
            callback=None
        )
        ```
       - Args:
          - inner_objective_feed_dicts: an optional feed dictionary for the inner problem. Can be a function of step, which accounts for, e.g. stochastic gradient descent.
          - outer_objective_feed_dicts: an optional feed dictionary for the outer optimization problem (passed to the evaluation of outer objective). Can be a function of hyper-iterations steps (i.e. global variable), which may account for, e.g. stochastic evaluation of outer objective.
          - train_batches: used for Reptile Algorithm, which needs to generates mini batches of images and labels during one training step
          - initializer_feed_dict:  an optional feed dictionary for the initialization of inner problems variables. Can be a function of hyper-iterations steps (i.e. global variable), which may account for, e.g. stochastic initialization.
          - session: optional session
          - online: default `False` if `True` performs the online version of the algorithms (i.e. does not reinitialize the state after at each run).
          - callback: optional callback function of signature (step (int), feed_dictionary, `tf.Session`) -> None that are called after every forward iteration.
       - Returns: None
  - Simple Running Example <div3 id="a33"></div3>
    ```
        from boml import utils
        from boml.script_helper import *
        dataset = boml.meta_omniglot(args.num_classes, (args.examples_train, args.examples_test))
        ex = boml.BOMLExperiment(dataset)
        # build network structure and define hyperparameters
        boml_ho = boml.BOMLOptimizer('HyperOptim', 'Aggr', 'Reverse')
        meta_learner = boml_ho.Meta_learner(ex.x, dataset, 'v1', args.use_T)
        ex.model = boml_ho.Base_learner(meta_learner.out, meta_learner)
        # define Lower-level problems
        loss_inner = utils.cross_entropy_loss(ex.model.out, ex.y)
        inner_grad = boml_ho.LL_problem(loss_inner, args.lr, args.T, experiment=ex)
        # define Upper-level problems
        loss_outer = utils.cross_entropy_loss(ex.model.re_forward(ex.x_).out, ex.y_)
        boml_ho.UL_problem(loss_outer, args.mlr, inner_grad, hyper_list=boml.extension.hyperparameters())
        boml_ho.Aggregate_all()
        # meta training step
        with utils.get_default_session():
            for itr in range(args.meta_train_iterations):
                train_batch = BatchQueueMock(dataset.train, 1,args.meta_batch_size，utils.get_rand_state())
                tr_fd, v_fd = feed_dicts(train_batch)
                boml_ho.run(tr_fd, v_fd)
    ``` 